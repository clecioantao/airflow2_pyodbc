# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# Basic Airflow cluster configuration for CeleryExecutor with Redis and PostgreSQL.
#
# WARNING: This configuration is for local development. Do not use it in a production deployment.
#
# This configuration supports basic configuration using environment variables or an .env file
# The following variables are supported:
#
# AIRFLOW_IMAGE_NAME         - Docker image name used to run Airflow.
#                              Default: apache/airflow:master-python3.8
# AIRFLOW_UID                - User ID in Airflow containers
#                              Default: 50000
# AIRFLOW_GID                - Group ID in Airflow containers
#                              Default: 50000
# _AIRFLOW_WWW_USER_USERNAME - Username for the administrator account.
#                              Default: airflow
# _AIRFLOW_WWW_USER_PASSWORD - Password for the administrator account.
#                              Default: airflow
#
# Feel free to modify this file to suit your needs.
#
# And Ozbay's note: The original file was modified to reduce the footprint. The original yml
# file can be found at https://airflow.apache.org/docs/apache-airflow/2.0.1/start/docker.html
#
---
version: '3.8'
x-airflow-common:
  &airflow-common
  image: ${AIRFLOW_IMAGE_NAME:-clecio/airflow_pyodbc:2.0.2.1}
  environment:
    &airflow-common-env
    INSTALL_MYSQL: y
    LOAD_EX: n
    EXECUTOR: Local
    AIRFLOW__SMTP__SMTP_HOST: smtp.gmail.com
    AIRFLOW__SMTP__SMTP_USER: clecio.antao@gmail.com
    AIRFLOW__SMTP__SMTP_PASSWORD: 'Proteu690201@'
    AIRFLOW__SMTP__SMTP_PORT: 465
    AIRFLOW__SMTP__SMTP_MAIL_FROM: Airflow
    AIRFLOW__CORE__MIN_SERIALIZED_DAG_UPDATE_INTERVAL: 0
    AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 30
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: 'false'
    AIRFLOW__CORE__DEFAULT_TIMEZONE: 'America/Sao_Paulo'
    AIRFLOW__WEBSERVER__WORKERS: 2
  volumes:
    - ./config:/opt/airflow/config
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./init:/opt/airflow/init
    - ./secrets:/opt/airflow/secrets
    - ./plugins:/opt/airflow/plugins
    - ./sql_files:/opt/airflow/sql_files
  user: "${AIRFLOW_UID:-50000}:${AIRFLOW_GID:-50000}"
  depends_on:
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:13
    container_name: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
      - ./pginit:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 30s
      retries: 5
    restart: always

  pgadmin4:
    container_name: pgadmin4
    image: dpage/pgadmin4
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL:-clecio.antao@gmail.com.br}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD:-Proteu690201}
      PGADMIN_CONFIG_SERVER_MODE: 'False'
    ports:
      - 8001:80
    depends_on:
      - postgres    
    restart: always      
    volumes:
      - pgadmin:/root/.pgadmin
    restart: always

  # mysql:
  #   image: mysql:5.6
  #   container_name: mysql
  #   ports:
  #     - "3306:3306"
  #   volumes:
  #     - ./dags/data:/store_files_mysql/
  #     - ./mysql.cnf:/etc/mysql/mysql.cnf
  #   environment:
  #     - MYSQL_ROOT_PASSWORD=root
  #     - MYSQL_DATABASE=app_development
  #   restart: always

#   # MySQL
#   mysql:
#     container_name: mysql
#     image: mysql:8.0
#     command: mysqld --default-authentication-plugin=mysql_native_password --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci
#     environment:
#       MYSQL_ROOT_PASSWORD: root
# #      MYSQL_DATABASE: sandbox
# #      MYSQL_USER: sandbox_user
# #      MYSQL_PASSWORD: passpass
#       MYSQL_ALLOW_EMPTY_PASSWORD: "yes"
#     ports:
#       - '3306:3306'
#     # volumes:
#     #   - './db/data:/var/lib/mysql'
#     #   - './db/my.cnf:/etc/mysql/conf.d/my.cnf'
#     #   - './db/sql:/docker-entrypoint-initdb.d'  
#     restart: always

  sqlserver1:
      image: 'microsoft/mssql-server-linux' 
      container_name: mssql_bd
      ports:  
        - "1434:1433"
      environment:
        - ACCEPT_EULA=Y
        - SA_PASSWORD=Proteu690201
      volumes: 
        - sqlsystem:/var/opt/mssql/
        - sqldata:/var/opt/sqlserver/data
        - sqllog:/var/opt/sqlserver/log
        - sqlbackup:/var/opt/sqlserver/backup
      restart: always

  airflow-webserver:
    <<: *airflow-common
    container_name: airflow
    # command: webserver
    entrypoint: /opt/airflow/init/start_airflow.sh
    ports:
      - 8082:8080
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://192.168.2.155:8082/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: always

  airflow-init:
    <<: *airflow-common
    container_name: airflow_init
    command: version
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}

  cloudera:
    image: cloudera/quickstart:latest
    container_name: cloudera
    privileged: true
    hostname: quickstart.cloudera
    command: /usr/bin/docker-quickstart
    ports:
      - "8020:8020"   # HDFS 
      - "8022:22"     # SSH
      - "7180:7180"   # Cloudera Manager
      - "8888:8888"   # Hue
      - "11000:11000" # Oozie
      - "50070:50070" # HDFS Rest Namenode
      - "50075:50075" # HDFS Rest Datanode
      - "2181:2181"   # Zookeeper
      - "8088:8088"   # YARN Resource Manager
      - "19888:19888" # MapReduce Job History
      - "50030:50030" # MapReduce Job Tracker
      - "8983:8983"   # Solr
      - "16000:16000" # Sqoop Metastore
      - "8042:8042"   # YARN Node Manager
      - "60010:60010" # HBase Master
      - "60030:60030" # HBase Region
      - "9091:9090"   # HBase Thrift
      - "8089:8080"   # HBase Rest
      - "7077:7077"   # Spark Master
    tty: true
    stdin_open: true
    volumes: 
      - /var/shared_cloudera_quickstart:/media/shared_from_local

  # grafana:
  #   container_name: grafana
  #   image: grafana/grafana:7.5.6 # 6.1.2  # 7.4.5 # 6.7.6 # 7.1.3 # 7.5.6
  #   ports:
  #     - 3000:3000
  #   volumes: 
  #     - grafana-storage:/var/lib/grafana
  #   environment: 
  #     GF_SMTP_ENABLED: "true"
  #     GF_SMTP_HOST: "smtp.gmail.com:465"
  #     GF_SMTP_USER: "clecio.antao@gmail.com"
  #     GF_SMTP_PASSWORD: "Proteu690201@"
  #     GF_SMTP_SKIP_VERIFY: "true"
  #     GF_SMTP_FROM_ADDRESS: "clecio.antao@gmail.com"
  #     GF_SMTP_FROM_NAME: "clecio.antao@gmail.com"


  # graphite-statsd:
  #   image: graphiteapp/graphite-statsd
  #   ports:
  #     - 2003-2004:2003-2004
  #     - 2023-2024:2023-2024
  #     - 8125:8125/udp
  #     - 8126:8126
  #     - 8090:80     
  #   volumes: 
  #     - ./graph_config:/opt/graphite/conf
  #     - graphite:/opt/graphite/storage/whisper
  #   restart: always

  # elasticsearch:
  #   container_name: elasticsearch
  #   image: docker.elastic.co/elasticsearch/elasticsearch:7.10.0
  #   restart: unless-stopped
  #   ports:
  #     - "9200:9200"
  #     - "9300:9300"
  #   environment:
  #     - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
  #     - discovery.type=single-node
  #   ulimits:
  #     memlock:
  #       soft: -1
  #       hard: -1
  #   volumes:
  #     - esdata01:/usr/share/elasticsearch/data

  # kibana:
  #   image: docker.elastic.co/kibana/kibana:7.10.0
  #   container_name: kibana
  #   restart: unless-stopped
  #   ports:
  #     - "5601:5601"
  #   environment:
  #     - SERVER_NAME=kibana.local
  #     - ELASTICSEARCH_URL=http://elasticsearch:9200
  #   depends_on:
  #     - elasticsearch

volumes:
  postgres-db-volume:
  data:
  pgadmin:
  sqlsystem:
  sqldata:
  sqllog:
  sqlbackup:    
  esdata01:
    driver: local
